{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we show PROMIS Approximation application in the CRIME dataset, where the audit regions are in total 8 generated from KMeans. \n",
    "\n",
    "We show through visualization the initial spatial bias and the results of the mitigation proccess by using the PROMIS approach to adjust the decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(\"../..\")))\n",
    "\n",
    "from methods.models.optimization_model import SpatialOptimFairnessModel\n",
    "from utils.data_utils import read_scanned_regs, get_y, get_pos_info_regions\n",
    "from utils.results_names_utils import combine_world_info, get_train_val_test_paths\n",
    "import pandas as pd\n",
    "from utils.scores import get_sbi, get_fair_stat_ratios\n",
    "# from utils.stats_utils import get_signif_threshold\n",
    "from utils.audit_utils import get_signif_thresh_scanned_regions\n",
    "from utils.plot_utils import plot_fairness_map, plot_thresholds_adjustments\n",
    "import numpy as np\n",
    "from utils.geo_utils import compute_polygons, filterbbox \n",
    "from sklearn.metrics import accuracy_score\n",
    "from utils.plot_utils import plot_map_with_polygons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "base_path = \"../../../data/\"\n",
    "clf_name = \"xgb\"\n",
    "dataset_name = \"crime\"\n",
    "partioning_type_name = \"non_overlap_k_8\"\n",
    "fairness_notion = \"equal_opportunity\"\n",
    "\n",
    "results = {}\n",
    "res_desc_label, partioning_name, prediction_name = combine_world_info(\n",
    "    dataset_name, partioning_type_name, clf_name\n",
    ")\n",
    "_, val_path_info, test_path_info = get_train_val_test_paths(\n",
    "    base_path, partioning_name, prediction_name, dataset_name\n",
    ")\n",
    "val_regions_df = read_scanned_regs(val_path_info[\"regions\"])\n",
    "val_pred_df = pd.read_csv(val_path_info[\"predictions\"])\n",
    "val_labels_df = pd.read_csv(val_path_info[\"labels\"])\n",
    "y_pred_val = get_y(val_pred_df, \"pred\")\n",
    "y_pred_probs_val = get_y(val_pred_df, \"prob\")\n",
    "y_true_val = get_y(val_labels_df, \"label\")\n",
    "test_regions_df = read_scanned_regs(test_path_info[\"regions\"])\n",
    "test_pred_df = pd.read_csv(test_path_info[\"predictions\"])\n",
    "test_labels_df = pd.read_csv(test_path_info[\"labels\"])\n",
    "y_pred_test = get_y(test_pred_df, \"pred\")\n",
    "y_pred_probs_test = get_y(test_pred_df, \"prob\")\n",
    "y_true_test = get_y(test_labels_df, \"label\")\n",
    "val_points_per_region = val_regions_df[\"points\"].tolist()\n",
    "test_points_per_region = test_regions_df[\"points\"].tolist()\n",
    "\n",
    "# keep instances with positive labels (for equal opportunity)\n",
    "pos_y_true_indices_test, pos_points_per_region_test = get_pos_info_regions(\n",
    "    y_true_test, test_points_per_region\n",
    ")\n",
    "y_pred_pos_test = y_pred_test[pos_y_true_indices_test]\n",
    "pos_test_regions_df = test_regions_df.copy()\n",
    "pos_test_regions_df['points'] = pos_points_per_region_test\n",
    "\n",
    "\n",
    "accuracy_test_init = accuracy_score(y_true_test, y_pred_test)\n",
    "print(f\"Total regions: {len(test_regions_df)}\")\n",
    "print(f\"Total test instances: {len(y_pred_test)}\")\n",
    "print(f\"Test accuracy: {accuracy_test_init:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"=\"*50)\n",
    "# print(\"ðŸ§® Input for Fairness Audit\")\n",
    "# print(\"=\"*50)\n",
    "# print(f\"  â€¢ Y Prediction        : {y_pred_test[:5]}\")\n",
    "# print(f\"  â€¢ Y True              : {y_true_test[:5]}\")\n",
    "# print(f\"  â€¢ Regions[0] (test)   : {test_points_per_region[0][:5]}\")\n",
    "\n",
    "# print(\"\\nðŸ“Œ Audit Hyperparameters:\")\n",
    "# print(f\"  â€¢ Fairness Notion           : Equal Opportunity\")\n",
    "# print(f\"  â€¢ Significance Threshold    : 0.005\")\n",
    "# print(f\"  â€¢ Number of Alternate Worlds: 400\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute initial statistics\n",
    "\n",
    "N = len(y_pred_pos_test)\n",
    "P = np.sum(y_pred_pos_test)\n",
    "print(f'N={N} positives')\n",
    "print(f'P={P} true positives') #positives being 'serious crimes' == 1 and negative class: 'non-serious' crimes = 0 (predicted by RF classifier)\n",
    "test_pred_df.head()\n",
    "df_scanned_regs, signif_thresh = get_signif_thresh_scanned_regions(0.005, 400, test_points_per_region, y_pred_test, y_true=y_true_test, seed=42)  \n",
    "sbi_test = np.mean(df_scanned_regs['statistic'])\n",
    "test_regions_df['stat'] = df_scanned_regs['statistic']\n",
    "print(f\"Test SBI (Equal Opportunity): {sbi_test:.4f}\")\n",
    "print(f\"Test Significance Threshold: {signif_thresh:.4f}\")\n",
    "total_signif_regions = len(df_scanned_regs[df_scanned_regs['signif']])\n",
    "print(f\"Total Significant Regions: {total_signif_regions}\")\n",
    "display(df_scanned_regs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine bounding box for display to avoid plotting outliers\n",
    "\n",
    "bbox_min_lon=-118.6673\n",
    "bbox_min_lat=33.707\n",
    "bbox_max_lon=-118.16\n",
    "bbox_max_lat=34.3374\n",
    "\n",
    "# keep instances with positive labels (for equal opportunity)\n",
    "\n",
    "pos_test_regions_df = test_regions_df.copy()\n",
    "pos_test_regions_df['points'] = pos_points_per_region_test\n",
    "\n",
    "pos_test_regions_df['pos_pr'] = pos_test_regions_df['points'].apply(lambda pts: sum(y_pred_pos_test[pts])/len(pts) if len(pts) > 0 else 0)\n",
    "PR_test = sum(y_pred_pos_test)/len(y_pred_pos_test)\n",
    "\n",
    "pos_test_regions_df[\"fair_stat_ratio\"], max_stat_test = get_fair_stat_ratios(\n",
    "    pos_test_regions_df[\"stat\"].to_numpy(),\n",
    "    pos_test_regions_df[\"pos_pr\"].to_numpy(),\n",
    "    PR_test,\n",
    ")\n",
    "\n",
    "# keep instances in bounding box for display\n",
    "\n",
    "sub_df = test_pred_df[test_pred_df.index.isin(pos_y_true_indices_test)]\n",
    "sub_df = sub_df.reset_index(drop=True)\n",
    "\n",
    "sub_df = filterbbox(sub_df, bbox_min_lon, bbox_min_lat, bbox_max_lon, bbox_max_lat)\n",
    "sub_test_pos_regions_df = pos_test_regions_df.copy()\n",
    "set_new_pts = set(sub_df.index.tolist())\n",
    "sub_test_pos_regions_df['points'] = sub_test_pos_regions_df['points'].apply(lambda pts: list(set(pts) & set_new_pts))\n",
    "\n",
    "old_2_new_idx = {}\n",
    "for i, ind in enumerate(sub_df.index):\n",
    "    old_2_new_idx[ind] = i\n",
    "\n",
    "sub_df = sub_df.reset_index(drop=True)\n",
    "sub_test_pos_regions_df['points'] = sub_test_pos_regions_df['points'].apply(lambda pts: [old_2_new_idx[p] for p in pts])\n",
    "sub_test_pos_regions_df = compute_polygons(sub_test_pos_regions_df, sub_df)\n",
    "\n",
    "y_pred_test_sub = get_y(sub_df, \"pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # show the regions and the points of the unfair by design world\n",
    "# plot_map_with_polygons(\n",
    "#     df=sub_df,\n",
    "#     y_pred=y_pred_test_sub,\n",
    "#     regs_df_list=[sub_test_pos_regions_df],\n",
    "#     regs_color_list=[\"#0000FF\"],\n",
    "#     title=\"Regions - Points - Base Model\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polygons = sub_test_pos_regions_df['polygon'].tolist()\n",
    "# polygons_ = [[list(pt) for pt in poly] for poly in polygons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # Construct indiv_info list\n",
    "# indiv_info = [\n",
    "#     {\n",
    "#         \"y_pred\": int(y_pred_test[i]),\n",
    "#         \"y_true\": int(y_true_test[i]),\n",
    "#         \"x\": None,\n",
    "#         \"y\": None\n",
    "#     }\n",
    "#     for i in range(len(y_pred_test))\n",
    "# ]\n",
    "\n",
    "# # region_indices: list of lists of individual indices\n",
    "# region_indices = test_points_per_region  # already in required format\n",
    "\n",
    "# # Full audit payload\n",
    "# audit_payload = {\n",
    "#     \"n_worlds\": 400,\n",
    "#     \"signif_level\": 0.005,\n",
    "#     \"equal_opp\": True,\n",
    "#     \"indiv_info\": indiv_info,\n",
    "#     \"region_info\": [\n",
    "#         {\n",
    "#             \"polygon\": poly,\n",
    "#         }\n",
    "#         for poly in polygons_\n",
    "#     ],\n",
    "#     \"region_indices\": region_indices\n",
    "# }\n",
    "\n",
    "# # Pretty-print to inspect or export to JSON file\n",
    "# print(json.dumps(audit_payload, indent=2))\n",
    "\n",
    "# # Optional: save to file\n",
    "# with open(\"audit_input.json\", \"w\") as f:\n",
    "#     json.dump(audit_payload, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shouls normalized LR\n",
    "plot_fairness_map(\n",
    "    regs_df_list=[sub_test_pos_regions_df],\n",
    "    title=\"XGBoost Predictions - Normalized LR\",\n",
    "    score_label=\"fair_stat_ratio\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the original thresholds and the respective normalized LR with colors\n",
    "figsize = (12, 6)\n",
    "plot_thresholds_adjustments(\n",
    "    thresholds=[0.5]*len(test_points_per_region),  \n",
    "    region_sizes=sub_test_pos_regions_df['fair_stat_ratio'].to_numpy(),\n",
    "    figsize=figsize,\n",
    "    display_title=True,\n",
    "    title=\"Classification Threshold per Region of the XGBoost Model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"=\"*50)\n",
    "# print(\"ðŸ§ª Input for Mitigation\")\n",
    "# print(\"=\"*50)\n",
    "# print(\"ðŸ”¹ Data for fitting the mitigator:\")\n",
    "# print(f\"  â€¢ Y (val) Prediction              : {y_pred_val[:5]}\")\n",
    "# print(f\"  â€¢ Y (val) Prediction Probabilities: {y_pred_probs_val[:5]}\")\n",
    "# print(f\"  â€¢ Y (val) True                    : {y_true_val[:5]}\")\n",
    "# print(f\"  â€¢ Regions[0] (val)                : {val_points_per_region[0][:5]}\")\n",
    "\n",
    "# print(\"\\nðŸ“Œ Hyperparameters:\")\n",
    "# print(f\"  â€¢ Mitigator                      : PROMIS-A (Approximation)\")\n",
    "# print(f\"  â€¢ Fairness Notion               : Equal Opportunity\")\n",
    "# print(f\"  â€¢ Budget                        : 5000\")\n",
    "# print(f\"  â€¢ Max Positive Ratio Shift      : 0.1\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*50)\n",
    "# print(\"ðŸ§¾ Data for Prediction with the Mitigated Model\")\n",
    "# print(\"=\"*50)\n",
    "# print(f\"  â€¢ Regions[0] (test)              : {test_points_per_region[0][:5]}\")\n",
    "# print(f\"  â€¢ Y (test) Prediction Probabilities: {y_pred_probs_test[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pts_per_region_to_indiv_info(pts_per_region):\n",
    "#     \"\"\"\n",
    "#     Convert points per region to individual info format.\n",
    "    \n",
    "#     Args:\n",
    "#         pts_per_region (list of list): List of lists where each sublist contains indices of points in a region.\n",
    "        \n",
    "#     Returns:\n",
    "#         list: List of dictionaries with individual info.\n",
    "#     \"\"\"\n",
    "#     indiv_to_regions = {}\n",
    "#     for reg_id, pts in enumerate(pts_per_region):\n",
    "#         for pt in pts:\n",
    "#             if pt not in indiv_to_regions:\n",
    "#                 indiv_to_regions[pt] = []\n",
    "#             indiv_to_regions[pt].append(reg_id)\n",
    "#     indiv_ids_sorted = sorted(indiv_to_regions.keys())\n",
    "#     indiv_regions_list = [indiv_to_regions[ind] for ind in indiv_ids_sorted]\n",
    "#     return indiv_regions_list\n",
    "\n",
    "# indiv_regions_ids_list_val = pts_per_region_to_indiv_info(val_points_per_region)\n",
    "# indiv_regions_ids_list_test = pts_per_region_to_indiv_info(test_points_per_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# val_lats = val_pred_df[\"lat\"].values.tolist() \n",
    "# val_lons = val_pred_df[\"lon\"].values.tolist()\n",
    "# test_lats = test_pred_df[\"lat\"].values.tolist()\n",
    "# test_lons = test_pred_df[\"lon\"].values.tolist()\n",
    "# # ðŸ‘‰ Construct the MitigationRequest payload for threshold-based mitigation\n",
    "# payload = {\n",
    "#     \"fit_indiv_info\": [\n",
    "#         {\n",
    "#             \"y_pred\": int(y_pred_val[i]),\n",
    "#             \"y_pred_prob\": float(y_pred_probs_val[i]),\n",
    "#             \"lat\": val_lats[i],\n",
    "#             \"lon\": val_lons[i],\n",
    "#             \"y_true\": int(y_true_val[i]),\n",
    "#             \"region_ids\": indiv_regions_ids_list_val[i] \n",
    "#         }\n",
    "#         for i in range(len(y_pred_val))\n",
    "#     ],\n",
    "#     \"predict_indiv_info\": [\n",
    "#         {\n",
    "#             \"y_pred\": int(y_pred_test[i]),\n",
    "#             \"y_pred_prob\": float(y_pred_probs_test[i]),\n",
    "#             \"lat\": test_lats[i],\n",
    "#             \"lon\": test_lons[i],\n",
    "#             \"y_true\": int(y_true_test[i]),\n",
    "#             \"region_ids\": indiv_regions_ids_list_test[i]\n",
    "#         }\n",
    "#         for i in range(len(y_pred_probs_test))\n",
    "#     ],\n",
    "#     \"predict_region_info\": [{\"polygon\": poly} for poly in polygons_],\n",
    "# }\n",
    "\n",
    "# # ðŸ”½ Save each component to its own file\n",
    "# with open(\"seperate_threshold_input_coords/fit_indiv_info.json\", \"w\") as f:\n",
    "#     json.dump(payload[\"fit_indiv_info\"], f, indent=2)\n",
    "\n",
    "# with open(\"seperate_threshold_input_coords/predict_indiv_info.json\", \"w\") as f:\n",
    "#     json.dump(payload[\"predict_indiv_info\"], f, indent=2)\n",
    "\n",
    "# with open(\"seperate_threshold_input_coords/predict_region_info.json\", \"w\") as f:\n",
    "#     json.dump(payload[\"predict_region_info\"], f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply PROMIS Approximation mitigation method (equal opportunity)\n",
    "fair_model = SpatialOptimFairnessModel(\"promis_app\")\n",
    "fair_model.fit(\n",
    "    points_per_region=val_points_per_region,\n",
    "    y_pred=y_pred_val,\n",
    "    y_true=y_true_val,\n",
    "    y_pred_probs=y_pred_probs_val,\n",
    "    budget=0.1, # 5000\n",
    "    max_pr_shift=0.1,\n",
    "    wlimit=300,\n",
    "    fair_notion=fairness_notion,\n",
    "    overlap=True,\n",
    "    no_of_threads=0,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Apply the new thresholds and get the new predictions\n",
    "test_new_preds = fair_model.predict(test_points_per_region, y_pred_probs_test, apply_fit_flips=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep instances with positive labels (for equal opportunity)\n",
    "# and compute new statistics\n",
    "test_new_preds_pos = test_new_preds[pos_y_true_indices_test]\n",
    "N_test_new = len(test_new_preds_pos)\n",
    "P_test_new = np.sum(test_new_preds_pos)\n",
    "print(f'N={N_test_new} points')\n",
    "print(f'P={P_test_new} positives') #positives being 'serious crimes' == 1 and negative class: 'non-serious' crimes = 0 (predicted by RF classifier)\n",
    "\n",
    "df_scanned_regs, signif_thresh_test = get_signif_thresh_scanned_regions(0.005, 400, test_points_per_region, test_new_preds, y_true_test, seed=42)  \n",
    "sbi_test = np.mean(df_scanned_regs['statistic'])\n",
    "pos_test_regions_df['new_stat'] = df_scanned_regs['statistic']\n",
    "\n",
    "print(f\"Test SBI (Equal Opportunity): {sbi_test:.3f}\")\n",
    "print(f\"Test Significance Threshold: {signif_thresh_test:.5f}\")\n",
    "\n",
    "total_signif_regions = len(df_scanned_regs[df_scanned_regs['signif']])\n",
    "print(f\"Total Significant Regions: {total_signif_regions}\")\n",
    "display(df_scanned_regs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute normalized LR and keep instances in bounding box for display\n",
    "pos_test_regions_df['new_pos_pr'] = pos_test_regions_df['points'].apply(lambda pts: sum(test_new_preds_pos[pts])/len(pts))\n",
    "PR_test_new = sum(test_new_preds_pos)/len(test_new_preds_pos)\n",
    "\n",
    "pos_test_regions_df[\"new_fair_stat_ratio\"], _ = get_fair_stat_ratios(\n",
    "    pos_test_regions_df[\"new_stat\"].to_numpy(),\n",
    "    pos_test_regions_df[\"new_pos_pr\"].to_numpy(),\n",
    "    PR_test_new,\n",
    "    max_stat_test\n",
    ")\n",
    "sub_test_pos_regions_df['new_fair_stat_ratio'] = pos_test_regions_df[\"new_fair_stat_ratio\"]\n",
    "sub_test_pos_regions_df['new_stat'] = pos_test_regions_df[\"new_stat\"]\n",
    "sub_test_pos_regions_df['new_pos_pr'] = pos_test_regions_df[\"new_pos_pr\"]\n",
    "sub_test_pos_regions_df[[\"pos_pr\", \"new_pos_pr\", \"fair_stat_ratio\", \"new_fair_stat_ratio\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the original thresholds and the respective normalized LR with colors\n",
    "figsize = (12, 6)\n",
    "plot_thresholds_adjustments(\n",
    "    thresholds=fair_model.thresholds,  # List of threshold values\n",
    "    region_sizes=sub_test_pos_regions_df['new_fair_stat_ratio'].to_numpy(),\n",
    "    figsize=figsize,\n",
    "    display_title=True,\n",
    "    title=\"Classification Thresholds per Region After Mitigation\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the new normalized LR after mitigation\n",
    "plot_fairness_map(\n",
    "    regs_df_list=[sub_test_pos_regions_df],\n",
    "    title=\"Mitigated Predictions - Normalized LR\",\n",
    "    score_label=\"new_fair_stat_ratio\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_true_test, test_new_preds)\n",
    "print(f\"Test accuracy before mitigation: {accuracy_test_init:.3f}\")\n",
    "print(f\"Test accuracy after mitigation: {accuracy:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PROMIS Env",
   "language": "python",
   "name": "promis-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
