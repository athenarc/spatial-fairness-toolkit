{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook performs the experiment, where a DNN is the base model to mitigate. We consider two fairness definitions: statistical parity and equal opportunity. All three compared methods are assessed on statistical parity, while only FairWhere and PROMIS are assessed on equal opportunity. For FairWhere we define the distance for each partition as the absoluted difference of the partition recall and the overall model recall and for statistical parity we use the positive ratio as metric\n",
    "The three types of audit regions are tested and with the 2 fairness notions result in 6 experiments.\n",
    "\n",
    "Initially the base DNN is trained. FairWhere continues training on the training set with the goal of mitigating spatial bias. At the start of fairness training, each sampled partitioning is used for 5 epochs before transitioning to the next, iterating over 120 different samples (equivalent to five full enumerations over all 24 partitioning candidates). Subsequently, each epoch samples a new partitioning, continuing for 720 samples (equivalent to thirty full enumerations). In total, each partitioning is expected to be used for 50 epochs throughout\n",
    "the training process.\n",
    "\n",
    "The predictions of the base and corrected (by FairWhere) model are saved for later analysis.\n",
    "\n",
    "The maximum budget, given as input for PROMIS and SpatialFlips is computed by the absolute difference of the predictions on the validation set between the base DNN and FairWhere models. \n",
    "\n",
    "PROMIS methods apply the thresholds adjustment approach using the validation set, while SpatialFlips apply flips directly on the test set.\n",
    "\n",
    "The \"pretrained\" PROMIS, SpatialFlip models are saved for later analysis on the test set.\n",
    "\n",
    "You may opt which experiment to run by uncommenting the relate part in the section choose experiment to run. To rerun an experiment restart the notebook (for reproducibility).\n",
    "\n",
    "For each experiment the directory dnn_exp/regions_\\<partitioning_name>|pred_dnn_crime is created.\n",
    "\n",
    "Inside the directory the following files are created:\n",
    "\n",
    "* for FairWhere:\n",
    "    * keras_models/\\<fairness_notion>_crime_partition_model.keras\n",
    "    * crime_\\<fairness_notion>_where_fit_time.txt\n",
    "    * crime_\\<fairness_notion>\\_where_model_\\<set>_pred.csv\n",
    "* for SpatialFlip:\n",
    "    * spatial_flip_models/\\<fairness_notion>/iter.pkl\n",
    "* for PROMIS:\n",
    "    * spatial_optim_models/\\<fairness_notion>/<promis_method_name>.pkl (for promis_opt method the wlimit_\\<work_limit> is appended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2287,
     "status": "ok",
     "timestamp": 1702284248589,
     "user": {
      "displayName": "ZHOU TIANHENG",
      "userId": "06545871880155544798"
     },
     "user_tz": -60
    },
    "id": "nfrZK-fWMprk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import recall_score,f1_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Sequential\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "from methods.models.optimization_model import SpatialOptimFairnessModel \n",
    "from methods.models.spatial_flip_model import SpatialFlipFairnessModel\n",
    "from utils.results_names_utils import combine_world_info\n",
    "from utils.data_utils import get_y, read_scanned_regs, get_pos_info_regions\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Experiment to Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equal Opportunity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audit Regions = Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fair_notion = \"equal_opportunity\" \n",
    "# partioning_type_name = \"5_x_5\"\n",
    "# n_flips_start = 400\n",
    "# overlap=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audit Regions = Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fair_notion = \"equal_opportunity\" \n",
    "# partioning_type_name = \"non_overlap_k_8\"\n",
    "# n_flips_start = 400\n",
    "# overlap=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audit Regions = Scan Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fair_notion = \"equal_opportunity\" \n",
    "# partioning_type_name = \"overlap_k_10_radii_4\"\n",
    "# n_flips_start = 1000\n",
    "# overlap=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Parity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audit Regions = Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fair_notion = \"statistical_parity\" \n",
    "# partioning_type_name = \"5_x_5\"\n",
    "# n_flips_start = 1200\n",
    "# overlap=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audit Regions = Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fair_notion = \"statistical_parity\" \n",
    "# partioning_type_name = \"non_overlap_k_8\"\n",
    "# n_flips_start = 900\n",
    "# overlap=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audit Regions = Scan Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_notion = \"statistical_parity\" \n",
    "partioning_type_name = \"overlap_k_10_radii_4\"\n",
    "n_flips_start = 3000\n",
    "overlap=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../data/\"\n",
    "prediction_dir = f'{data_path}predictions/' # folder in data path to save the base model predictions\n",
    "partitioning_dir = f'{data_path}partitionings/' # folder in data path to save the partitionings\n",
    "preprocess_dir = f'{data_path}preprocess/'\n",
    "dataset_name = \"crime\"\n",
    "clf_name = \"dnn\"\n",
    "results_path = \"../../results/dnn_exp/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_desc_label, partioning_name, prediction_name = combine_world_info(\n",
    "    dataset_name, partioning_type_name, clf_name\n",
    ")\n",
    "results_path = f\"{results_path}{res_desc_label}/\"\n",
    "base_model_fname = f\"{data_path}clf/dnn_{dataset_name}.keras\"\n",
    "keras_models_dir = f'{results_path}keras_models/{fair_notion}/' # folder to save base and where model\n",
    "sp_opt_models_dir = f'{results_path}spatial_optim_models/{fair_notion}/'\n",
    "for dir in [results_path, keras_models_dir]:\n",
    "    os.makedirs(dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORE_LABEL = 'recall' if fair_notion == 'equal_opportunity' else 'pr'\n",
    "print(f\"SCORE_LABEL: {SCORE_LABEL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    intersect = tf.reduce_sum(y_pred * y_true, axis=0) + K.epsilon()\n",
    "    denominator = tf.reduce_sum(y_pred, axis=0) + tf.reduce_sum(y_true, axis=0)\n",
    "    dice_scores = 2 * intersect / (denominator + tf.keras.backend.epsilon())\n",
    "    return 1 - dice_scores\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    loss = dice(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "BATCH_SIZE = 64*64\n",
    "LEARNING_RATE = 0.0001\n",
    "LEARNING_RATE_INIT = LEARNING_RATE\n",
    "EPOCH_TRAIN = 100\n",
    "\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_train = pd.read_csv(f\"{preprocess_dir}X_train_crime.csv\")\n",
    "X_val = pd.read_csv(f\"{preprocess_dir}X_val_crime.csv\")\n",
    "X_test = pd.read_csv(f\"{preprocess_dir}X_test_crime.csv\")\n",
    "y_train = pd.read_csv(f\"{preprocess_dir}y_train_crime.csv\")\n",
    "y_val = pd.read_csv(f\"{preprocess_dir}y_val_crime.csv\")\n",
    "y_test = pd.read_csv(f\"{preprocess_dir}y_test_crime.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_oRIUzSsoxy0"
   },
   "source": [
    "##  Train Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.keras.initializers.TruncatedNormal(stddev=0.5, seed=SEED)\n",
    "base_model = Sequential([\n",
    "    Dense(100, activation='elu', input_dim=X_train.shape[1], kernel_initializer=initializer),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.1),\n",
    "    Dense(50, activation='elu', kernel_initializer=initializer),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.1),\n",
    "    Dense(2, activation='softmax', kernel_initializer=initializer),\n",
    "])\n",
    "\n",
    "print(base_model.summary())\n",
    "\n",
    "\n",
    "optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "base_model.compile(optimizer=optimizer, loss=custom_loss, metrics=['accuracy'])\n",
    "\n",
    "# Start Training\n",
    "history = base_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCH_TRAIN,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_val, y_val),\n",
    ")\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the label\n",
    "y_train_pred_base = base_model.predict(X_train, batch_size=BATCH_SIZE)\n",
    "y_val_pred_base =  base_model.predict(X_val, batch_size=BATCH_SIZE)\n",
    "y_test_pred_base =  base_model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "#Change Softmax to class\n",
    "class_y_train_pred_base = np.argmax(y_train_pred_base, axis=-1)\n",
    "class_y_val_pred_base = np.argmax(y_val_pred_base, axis=-1)\n",
    "class_y_test_pred_base = np.argmax(y_test_pred_base, axis=-1)\n",
    "\n",
    "#Prediction Value Count\n",
    "print(np.unique(class_y_train_pred_base,return_counts=True))\n",
    "print(np.unique(class_y_val_pred_base,return_counts=True))\n",
    "print(np.unique(class_y_test_pred_base,return_counts=True))\n",
    "\n",
    "base_test_f1 = f1_score(y_test,class_y_test_pred_base)\n",
    "print(f\"F1 train: {f1_score(y_train,class_y_train_pred_base)}, F1 val: {f1_score(y_val,class_y_val_pred_base)}, F1 test: , {f1_score(y_test,class_y_test_pred_base)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Base Model Predictions and Generated Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_class_prob_idx = 1  # Positive class index\n",
    "\n",
    "y_train_base_pred_proba = base_model.predict(X_train, batch_size=BATCH_SIZE)[:, pos_class_prob_idx]\n",
    "y_val_base_pred_proba = base_model.predict(X_val, batch_size=BATCH_SIZE)[:, pos_class_prob_idx]\n",
    "y_test_base_pred_proba = base_model.predict(X_test, batch_size=BATCH_SIZE)[:, pos_class_prob_idx]\n",
    "\n",
    "preds_df = pd.DataFrame({\n",
    "    \"pred\": class_y_train_pred_base,\n",
    "    \"pred_proba\": y_train_base_pred_proba,\n",
    "})\n",
    "\n",
    "val_preds_df = pd.DataFrame({\n",
    "    \"pred\": class_y_val_pred_base,\n",
    "    \"prob\": y_val_base_pred_proba\n",
    "})\n",
    "\n",
    "test_preds_df = pd.DataFrame({\n",
    "    \"pred\": class_y_test_pred_base,\n",
    "    \"prob\": y_test_base_pred_proba\n",
    "})\n",
    "\n",
    "preds_df.to_csv(f\"{prediction_dir}train_{prediction_name}.csv\", index=False)\n",
    "val_preds_df.to_csv(f\"{prediction_dir}val_{prediction_name}.csv\", index=False)\n",
    "test_preds_df.to_csv(f\"{prediction_dir}test_{prediction_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Optimization Correction Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data for other spatial bias mitigation models\n",
    "val_pred_base_df = pd.read_csv(f\"{prediction_dir}val_{prediction_name}.csv\")\n",
    "val_pred_partition_df = pd.read_csv(\n",
    "    f\"{results_path}{dataset_name}_{fair_notion}_where_model_val_pred.csv\"\n",
    ")\n",
    "y_val_df = pd.read_csv(f\"{preprocess_dir}y_val_{dataset_name}.csv\")\n",
    "y_val = get_y(y_val_df, \"label\")\n",
    "pts_per_region_val_df = read_scanned_regs(\n",
    "    f\"{partitioning_dir}val_{partioning_name}.csv\"\n",
    ")\n",
    "pts_per_region_val = pts_per_region_val_df[\"points\"].tolist()\n",
    "y_val_pred_base = get_y(val_pred_base_df, \"pred\")\n",
    "y_val_base_pred_proba = get_y(val_pred_base_df, \"prob\")\n",
    "y_val_pred_partition = get_y(val_pred_partition_df, \"pred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Maximum Budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fair_notion == \"equal_opportunity\":\n",
    "    # Get positive class information to consider equal opportunity\n",
    "    pos_val_y_true_indices, _ = get_pos_info_regions(y_val, pts_per_region_val)\n",
    "    pos_val_y_pred_base = y_val_pred_base[pos_val_y_true_indices]\n",
    "    pos_y_val_pred_partition = y_val_pred_partition[pos_val_y_true_indices]\n",
    "\n",
    "# Calculate the differences between the base and partition model\n",
    "# to use it as budget for the mitigation models\n",
    "base_partition_diffs = (\n",
    "    np.sum(np.abs(y_val_pred_base - y_val_pred_partition))\n",
    "    if fair_notion == \"statistical_parity\"\n",
    "    else np.sum((np.abs(pos_val_y_pred_base - pos_y_val_pred_partition)))\n",
    ")\n",
    "print(\"\\nDifferences between base and partition in validation set:\")\n",
    "print(\"Number of different elements:\", base_partition_diffs)\n",
    "n_flips = base_partition_diffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the PROMIS methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlimit=300#3600\n",
    "max_pr_shift=0.1\n",
    "promis_methods = [\n",
    "    \"promis_app\",\n",
    "    \"promis_opt\",\n",
    "]\n",
    "\n",
    "for method in promis_methods:\n",
    "    fair_model = SpatialOptimFairnessModel(method)\n",
    "    fair_model.multi_fit(\n",
    "        points_per_region=pts_per_region_val,\n",
    "        n_flips_start = n_flips_start,\n",
    "        step=n_flips_start,\n",
    "        n_flips=n_flips,\n",
    "        y_pred=y_val_pred_base,\n",
    "        y_true=y_val if fair_notion == \"equal_opportunity\" else None,\n",
    "        y_pred_probs=y_val_base_pred_proba,\n",
    "        wlimit=wlimit,\n",
    "        fair_notion=fair_notion,\n",
    "        overlap=overlap,\n",
    "        no_of_threads=0,\n",
    "        verbose=1,\n",
    "        max_pr_shift=max_pr_shift\n",
    "    )\n",
    "\n",
    "    if method == \"promis_opt\":\n",
    "        model_save_file = f\"{sp_opt_models_dir}/{method}_wlimit_{wlimit}.pkl\"\n",
    "    else:\n",
    "        model_save_file = f\"{sp_opt_models_dir}/{method}.pkl\"\n",
    "\n",
    "    fair_model.save_model(model_save_file)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO8gqPzfYENP1b5eS/AbDTx",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PROMIS Env",
   "language": "python",
   "name": "promis-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
